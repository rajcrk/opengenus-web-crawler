{
  "_from": "node-webcrawler",
  "_id": "node-webcrawler@0.8.0",
  "_inBundle": false,
  "_integrity": "sha1-O3+BNsMbNw3tyEfys4/RnvwoQ58=",
  "_location": "/node-webcrawler",
  "_phantomChildren": {
    "async": "2.6.0",
    "aws4": "1.7.0",
    "bl": "1.1.2",
    "boolbase": "1.0.0",
    "chalk": "1.1.3",
    "combined-stream": "1.0.6",
    "commander": "2.15.1",
    "core-util-is": "1.0.2",
    "dom-serializer": "0.1.0",
    "domelementtype": "1.3.0",
    "entities": "1.1.1",
    "extend": "3.0.1",
    "forever-agent": "0.6.1",
    "inherits": "2.0.3",
    "is-my-json-valid": "2.17.2",
    "is-typedarray": "1.0.0",
    "isstream": "0.1.2",
    "json-stringify-safe": "5.0.1",
    "jsprim": "1.4.1",
    "mime-types": "2.1.18",
    "nth-check": "1.0.1",
    "oauth-sign": "0.8.2",
    "pinkie-promise": "2.0.1",
    "sshpk": "1.14.1",
    "stringstream": "0.0.5",
    "tough-cookie": "2.3.4"
  },
  "_requested": {
    "type": "tag",
    "registry": true,
    "raw": "node-webcrawler",
    "name": "node-webcrawler",
    "escapedName": "node-webcrawler",
    "rawSpec": "",
    "saveSpec": null,
    "fetchSpec": "latest"
  },
  "_requiredBy": [
    "#USER",
    "/"
  ],
  "_resolved": "https://registry.npmjs.org/node-webcrawler/-/node-webcrawler-0.8.0.tgz",
  "_shasum": "3b7f8136c31b370dedc847f2b38fd19efc28439f",
  "_spec": "node-webcrawler",
  "_where": "/Users/rajkumar/Desktop/open-genus",
  "author": {
    "name": "Mike Chen"
  },
  "bugs": {
    "url": "https://github.com/bda-research/node-webcrawler/issues"
  },
  "bundleDependencies": false,
  "dependencies": {
    "bottleneckp": "1.1.0",
    "charset-parser": "^0.2.0",
    "cheerio": "0.19.0",
    "generic-pool": "2.2.0",
    "iconv": "*",
    "iconv-lite": "0.4.8",
    "lodash": "3.8.0",
    "request": "2.74.0",
    "seenreq": "^0.1.7",
    "whacko": "0.19.1"
  },
  "deprecated": false,
  "description": "Crawler is a web spider written with Nodejs. It gives you the full power of jQuery on the server to parse a big number of pages as they are downloaded, asynchronously",
  "devDependencies": {
    "chai": "2.3.0",
    "jsdom": "3.1.2",
    "mocha": "2.2.5",
    "mocha-testdata": "1.1.0",
    "sinon": "1.14.1"
  },
  "directories": {
    "test": "tests"
  },
  "homepage": "https://github.com/bda-research/node-webcrawler",
  "keywords": [
    "dom",
    "javascript",
    "crawling",
    "spider",
    "scraper",
    "scraping",
    "jquery",
    "crawler"
  ],
  "license": "ISC",
  "main": "./lib/crawler.js",
  "name": "node-webcrawler",
  "optionalDependencies": {
    "iconv": "*"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/bda-research/node-webcrawler.git"
  },
  "scripts": {
    "test": "./node_modules/mocha/bin/mocha --reporter spec --bail --timeout 10000 tests/*.js"
  },
  "version": "0.8.0"
}
